{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP hand's on 101.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOWtQvY9V8gH+Ux5FoaArQd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aryamanjain036/NLP-Hand-s-ON/blob/master/NLP_hand's_on_101.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5CHjVa8VqOMw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7ORgrtZqj33",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0fe7ec94-9452-47ae-fec2-cf6724486f84"
      },
      "source": [
        "nltk.download()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NLTK Downloader\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> d\n",
            "\n",
            "Download which package (l=list; x=cancel)?\n",
            "  Identifier> l\n",
            "Packages:\n",
            "  [ ] abc................. Australian Broadcasting Commission 2006\n",
            "  [ ] alpino.............. Alpino Dutch Treebank\n",
            "  [ ] averaged_perceptron_tagger Averaged Perceptron Tagger\n",
            "  [ ] averaged_perceptron_tagger_ru Averaged Perceptron Tagger (Russian)\n",
            "  [ ] basque_grammars..... Grammars for Basque\n",
            "  [ ] biocreative_ppi..... BioCreAtIvE (Critical Assessment of Information\n",
            "                           Extraction Systems in Biology)\n",
            "  [ ] bllip_wsj_no_aux.... BLLIP Parser: WSJ Model\n",
            "  [ ] book_grammars....... Grammars from NLTK Book\n",
            "  [ ] brown............... Brown Corpus\n",
            "  [ ] brown_tei........... Brown Corpus (TEI XML Version)\n",
            "  [ ] cess_cat............ CESS-CAT Treebank\n",
            "  [ ] cess_esp............ CESS-ESP Treebank\n",
            "  [ ] chat80.............. Chat-80 Data Files\n",
            "  [ ] city_database....... City Database\n",
            "  [ ] cmudict............. The Carnegie Mellon Pronouncing Dictionary (0.6)\n",
            "  [ ] comparative_sentences Comparative Sentence Dataset\n",
            "  [ ] comtrans............ ComTrans Corpus Sample\n",
            "  [ ] conll2000........... CONLL 2000 Chunking Corpus\n",
            "  [ ] conll2002........... CONLL 2002 Named Entity Recognition Corpus\n",
            "Hit Enter to continue: \n",
            "  [ ] conll2007........... Dependency Treebanks from CoNLL 2007 (Catalan\n",
            "                           and Basque Subset)\n",
            "  [ ] crubadan............ Crubadan Corpus\n",
            "  [ ] dependency_treebank. Dependency Parsed Treebank\n",
            "  [ ] dolch............... Dolch Word List\n",
            "  [ ] europarl_raw........ Sample European Parliament Proceedings Parallel\n",
            "                           Corpus\n",
            "  [ ] floresta............ Portuguese Treebank\n",
            "  [ ] framenet_v15........ FrameNet 1.5\n",
            "  [ ] framenet_v17........ FrameNet 1.7\n",
            "  [ ] gazetteers.......... Gazeteer Lists\n",
            "  [ ] genesis............. Genesis Corpus\n",
            "  [ ] gutenberg........... Project Gutenberg Selections\n",
            "  [ ] ieer................ NIST IE-ER DATA SAMPLE\n",
            "  [ ] inaugural........... C-Span Inaugural Address Corpus\n",
            "  [ ] indian.............. Indian Language POS-Tagged Corpus\n",
            "  [ ] jeita............... JEITA Public Morphologically Tagged Corpus (in\n",
            "                           ChaSen format)\n",
            "  [ ] kimmo............... PC-KIMMO Data Files\n",
            "  [ ] knbc................ KNB Corpus (Annotated blog corpus)\n",
            "  [ ] large_grammars...... Large context-free and feature-based grammars\n",
            "                           for parser comparison\n",
            "Hit Enter to continue: \n",
            "  [ ] lin_thesaurus....... Lin's Dependency Thesaurus\n",
            "  [ ] mac_morpho.......... MAC-MORPHO: Brazilian Portuguese news text with\n",
            "                           part-of-speech tags\n",
            "  [ ] machado............. Machado de Assis -- Obra Completa\n",
            "  [ ] masc_tagged......... MASC Tagged Corpus\n",
            "  [ ] maxent_ne_chunker... ACE Named Entity Chunker (Maximum entropy)\n",
            "  [ ] maxent_treebank_pos_tagger Treebank Part of Speech Tagger (Maximum entropy)\n",
            "  [ ] moses_sample........ Moses Sample Models\n",
            "  [ ] movie_reviews....... Sentiment Polarity Dataset Version 2.0\n",
            "  [ ] mte_teip5........... MULTEXT-East 1984 annotated corpus 4.0\n",
            "  [ ] mwa_ppdb............ The monolingual word aligner (Sultan et al.\n",
            "                           2015) subset of the Paraphrase Database.\n",
            "  [ ] names............... Names Corpus, Version 1.3 (1994-03-29)\n",
            "  [ ] nombank.1.0......... NomBank Corpus 1.0\n",
            "  [ ] nonbreaking_prefixes Non-Breaking Prefixes (Moses Decoder)\n",
            "  [ ] nps_chat............ NPS Chat\n",
            "  [ ] omw................. Open Multilingual Wordnet\n",
            "  [ ] opinion_lexicon..... Opinion Lexicon\n",
            "  [ ] panlex_swadesh...... PanLex Swadesh Corpora\n",
            "  [ ] paradigms........... Paradigm Corpus\n",
            "  [ ] pe08................ Cross-Framework and Cross-Domain Parser\n",
            "                           Evaluation Shared Task\n",
            "Hit Enter to continue: \n",
            "  [ ] perluniprops........ perluniprops: Index of Unicode Version 7.0.0\n",
            "                           character properties in Perl\n",
            "  [ ] pil................. The Patient Information Leaflet (PIL) Corpus\n",
            "  [ ] pl196x.............. Polish language of the XX century sixties\n",
            "  [ ] porter_test......... Porter Stemmer Test Files\n",
            "  [ ] ppattach............ Prepositional Phrase Attachment Corpus\n",
            "  [ ] problem_reports..... Problem Report Corpus\n",
            "  [ ] product_reviews_1... Product Reviews (5 Products)\n",
            "  [ ] product_reviews_2... Product Reviews (9 Products)\n",
            "  [ ] propbank............ Proposition Bank Corpus 1.0\n",
            "  [ ] pros_cons........... Pros and Cons\n",
            "  [ ] ptb................. Penn Treebank\n",
            "  [ ] punkt............... Punkt Tokenizer Models\n",
            "  [ ] qc.................. Experimental Data for Question Classification\n",
            "  [ ] reuters............. The Reuters-21578 benchmark corpus, ApteMod\n",
            "                           version\n",
            "  [ ] rslp................ RSLP Stemmer (Removedor de Sufixos da Lingua\n",
            "                           Portuguesa)\n",
            "  [ ] rte................. PASCAL RTE Challenges 1, 2, and 3\n",
            "  [ ] sample_grammars..... Sample Grammars\n",
            "  [ ] semcor.............. SemCor 3.0\n",
            "Hit Enter to continue: \n",
            "  [ ] senseval............ SENSEVAL 2 Corpus: Sense Tagged Text\n",
            "  [ ] sentence_polarity... Sentence Polarity Dataset v1.0\n",
            "  [ ] sentiwordnet........ SentiWordNet\n",
            "  [ ] shakespeare......... Shakespeare XML Corpus Sample\n",
            "  [ ] sinica_treebank..... Sinica Treebank Corpus Sample\n",
            "  [ ] smultron............ SMULTRON Corpus Sample\n",
            "  [ ] snowball_data....... Snowball Data\n",
            "  [ ] spanish_grammars.... Grammars for Spanish\n",
            "  [ ] state_union......... C-Span State of the Union Address Corpus\n",
            "  [ ] stopwords........... Stopwords Corpus\n",
            "  [ ] subjectivity........ Subjectivity Dataset v1.0\n",
            "  [ ] swadesh............. Swadesh Wordlists\n",
            "  [ ] switchboard......... Switchboard Corpus Sample\n",
            "  [ ] tagsets............. Help on Tagsets\n",
            "  [ ] timit............... TIMIT Corpus Sample\n",
            "  [ ] toolbox............. Toolbox Sample Files\n",
            "  [ ] treebank............ Penn Treebank Sample\n",
            "  [ ] twitter_samples..... Twitter Samples\n",
            "  [ ] udhr2............... Universal Declaration of Human Rights Corpus\n",
            "                           (Unicode Version)\n",
            "  [ ] udhr................ Universal Declaration of Human Rights Corpus\n",
            "Hit Enter to continue: \n",
            "  [ ] unicode_samples..... Unicode Samples\n",
            "  [ ] universal_tagset.... Mappings to the Universal Part-of-Speech Tagset\n",
            "  [ ] universal_treebanks_v20 Universal Treebanks Version 2.0\n",
            "  [ ] vader_lexicon....... VADER Sentiment Lexicon\n",
            "  [ ] verbnet3............ VerbNet Lexicon, Version 3.3\n",
            "  [ ] verbnet............. VerbNet Lexicon, Version 2.1\n",
            "  [ ] webtext............. Web Text Corpus\n",
            "  [ ] wmt15_eval.......... Evaluation data from WMT15\n",
            "  [ ] word2vec_sample..... Word2Vec Sample\n",
            "  [ ] wordnet............. WordNet\n",
            "  [ ] wordnet_ic.......... WordNet-InfoContent\n",
            "  [ ] words............... Word Lists\n",
            "  [ ] ycoe................ York-Toronto-Helsinki Parsed Corpus of Old\n",
            "                           English Prose\n",
            "\n",
            "Collections:\n",
            "  [ ] all-corpora......... All the corpora\n",
            "  [ ] all-nltk............ All packages available on nltk_data gh-pages\n",
            "                           branch\n",
            "  [ ] all................. All packages\n",
            "  [ ] book................ Everything used in the NLTK Book\n",
            "  [ ] popular............. Popular packages\n",
            "Hit Enter to continue: \n",
            "  [ ] tests............... Packages for running tests\n",
            "  [ ] third-party......... Third-party data packages\n",
            "\n",
            "([*] marks installed packages)\n",
            "\n",
            "Download which package (l=list; x=cancel)?\n",
            "  Identifier> all\n",
            "    Downloading collection 'all'\n",
            "       | \n",
            "       | Downloading package abc to /root/nltk_data...\n",
            "       |   Unzipping corpora/abc.zip.\n",
            "       | Downloading package alpino to /root/nltk_data...\n",
            "       |   Unzipping corpora/alpino.zip.\n",
            "       | Downloading package biocreative_ppi to /root/nltk_data...\n",
            "       |   Unzipping corpora/biocreative_ppi.zip.\n",
            "       | Downloading package brown to /root/nltk_data...\n",
            "       |   Unzipping corpora/brown.zip.\n",
            "       | Downloading package brown_tei to /root/nltk_data...\n",
            "       |   Unzipping corpora/brown_tei.zip.\n",
            "       | Downloading package cess_cat to /root/nltk_data...\n",
            "       |   Unzipping corpora/cess_cat.zip.\n",
            "       | Downloading package cess_esp to /root/nltk_data...\n",
            "       |   Unzipping corpora/cess_esp.zip.\n",
            "       | Downloading package chat80 to /root/nltk_data...\n",
            "       |   Unzipping corpora/chat80.zip.\n",
            "       | Downloading package city_database to /root/nltk_data...\n",
            "       |   Unzipping corpora/city_database.zip.\n",
            "       | Downloading package cmudict to /root/nltk_data...\n",
            "       |   Unzipping corpora/cmudict.zip.\n",
            "       | Downloading package comparative_sentences to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping corpora/comparative_sentences.zip.\n",
            "       | Downloading package comtrans to /root/nltk_data...\n",
            "       | Downloading package conll2000 to /root/nltk_data...\n",
            "       |   Unzipping corpora/conll2000.zip.\n",
            "       | Downloading package conll2002 to /root/nltk_data...\n",
            "       |   Unzipping corpora/conll2002.zip.\n",
            "       | Downloading package conll2007 to /root/nltk_data...\n",
            "       | Downloading package crubadan to /root/nltk_data...\n",
            "       |   Unzipping corpora/crubadan.zip.\n",
            "       | Downloading package dependency_treebank to /root/nltk_data...\n",
            "       |   Unzipping corpora/dependency_treebank.zip.\n",
            "       | Downloading package dolch to /root/nltk_data...\n",
            "       |   Unzipping corpora/dolch.zip.\n",
            "       | Downloading package europarl_raw to /root/nltk_data...\n",
            "       |   Unzipping corpora/europarl_raw.zip.\n",
            "       | Downloading package floresta to /root/nltk_data...\n",
            "       |   Unzipping corpora/floresta.zip.\n",
            "       | Downloading package framenet_v15 to /root/nltk_data...\n",
            "       |   Unzipping corpora/framenet_v15.zip.\n",
            "       | Downloading package framenet_v17 to /root/nltk_data...\n",
            "       |   Unzipping corpora/framenet_v17.zip.\n",
            "       | Downloading package gazetteers to /root/nltk_data...\n",
            "       |   Unzipping corpora/gazetteers.zip.\n",
            "       | Downloading package genesis to /root/nltk_data...\n",
            "       |   Unzipping corpora/genesis.zip.\n",
            "       | Downloading package gutenberg to /root/nltk_data...\n",
            "       |   Unzipping corpora/gutenberg.zip.\n",
            "       | Downloading package ieer to /root/nltk_data...\n",
            "       |   Unzipping corpora/ieer.zip.\n",
            "       | Downloading package inaugural to /root/nltk_data...\n",
            "       |   Unzipping corpora/inaugural.zip.\n",
            "       | Downloading package indian to /root/nltk_data...\n",
            "       |   Unzipping corpora/indian.zip.\n",
            "       | Downloading package jeita to /root/nltk_data...\n",
            "       | Downloading package kimmo to /root/nltk_data...\n",
            "       |   Unzipping corpora/kimmo.zip.\n",
            "       | Downloading package knbc to /root/nltk_data...\n",
            "       | Downloading package lin_thesaurus to /root/nltk_data...\n",
            "       |   Unzipping corpora/lin_thesaurus.zip.\n",
            "       | Downloading package mac_morpho to /root/nltk_data...\n",
            "       |   Unzipping corpora/mac_morpho.zip.\n",
            "       | Downloading package machado to /root/nltk_data...\n",
            "       | Downloading package masc_tagged to /root/nltk_data...\n",
            "       | Downloading package moses_sample to /root/nltk_data...\n",
            "       |   Unzipping models/moses_sample.zip.\n",
            "       | Downloading package movie_reviews to /root/nltk_data...\n",
            "       |   Unzipping corpora/movie_reviews.zip.\n",
            "       | Downloading package names to /root/nltk_data...\n",
            "       |   Unzipping corpora/names.zip.\n",
            "       | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "       | Downloading package nps_chat to /root/nltk_data...\n",
            "       |   Unzipping corpora/nps_chat.zip.\n",
            "       | Downloading package omw to /root/nltk_data...\n",
            "       |   Unzipping corpora/omw.zip.\n",
            "       | Downloading package opinion_lexicon to /root/nltk_data...\n",
            "       |   Unzipping corpora/opinion_lexicon.zip.\n",
            "       | Downloading package paradigms to /root/nltk_data...\n",
            "       |   Unzipping corpora/paradigms.zip.\n",
            "       | Downloading package pil to /root/nltk_data...\n",
            "       |   Unzipping corpora/pil.zip.\n",
            "       | Downloading package pl196x to /root/nltk_data...\n",
            "       |   Unzipping corpora/pl196x.zip.\n",
            "       | Downloading package ppattach to /root/nltk_data...\n",
            "       |   Unzipping corpora/ppattach.zip.\n",
            "       | Downloading package problem_reports to /root/nltk_data...\n",
            "       |   Unzipping corpora/problem_reports.zip.\n",
            "       | Downloading package propbank to /root/nltk_data...\n",
            "       | Downloading package ptb to /root/nltk_data...\n",
            "       |   Unzipping corpora/ptb.zip.\n",
            "       | Downloading package product_reviews_1 to /root/nltk_data...\n",
            "       |   Unzipping corpora/product_reviews_1.zip.\n",
            "       | Downloading package product_reviews_2 to /root/nltk_data...\n",
            "       |   Unzipping corpora/product_reviews_2.zip.\n",
            "       | Downloading package pros_cons to /root/nltk_data...\n",
            "       |   Unzipping corpora/pros_cons.zip.\n",
            "       | Downloading package qc to /root/nltk_data...\n",
            "       |   Unzipping corpora/qc.zip.\n",
            "       | Downloading package reuters to /root/nltk_data...\n",
            "       | Downloading package rte to /root/nltk_data...\n",
            "       |   Unzipping corpora/rte.zip.\n",
            "       | Downloading package semcor to /root/nltk_data...\n",
            "       | Downloading package senseval to /root/nltk_data...\n",
            "       |   Unzipping corpora/senseval.zip.\n",
            "       | Downloading package sentiwordnet to /root/nltk_data...\n",
            "       |   Unzipping corpora/sentiwordnet.zip.\n",
            "       | Downloading package sentence_polarity to /root/nltk_data...\n",
            "       |   Unzipping corpora/sentence_polarity.zip.\n",
            "       | Downloading package shakespeare to /root/nltk_data...\n",
            "       |   Unzipping corpora/shakespeare.zip.\n",
            "       | Downloading package sinica_treebank to /root/nltk_data...\n",
            "       |   Unzipping corpora/sinica_treebank.zip.\n",
            "       | Downloading package smultron to /root/nltk_data...\n",
            "       |   Unzipping corpora/smultron.zip.\n",
            "       | Downloading package state_union to /root/nltk_data...\n",
            "       |   Unzipping corpora/state_union.zip.\n",
            "       | Downloading package stopwords to /root/nltk_data...\n",
            "       |   Unzipping corpora/stopwords.zip.\n",
            "       | Downloading package subjectivity to /root/nltk_data...\n",
            "       |   Unzipping corpora/subjectivity.zip.\n",
            "       | Downloading package swadesh to /root/nltk_data...\n",
            "       |   Unzipping corpora/swadesh.zip.\n",
            "       | Downloading package switchboard to /root/nltk_data...\n",
            "       |   Unzipping corpora/switchboard.zip.\n",
            "       | Downloading package timit to /root/nltk_data...\n",
            "       |   Unzipping corpora/timit.zip.\n",
            "       | Downloading package toolbox to /root/nltk_data...\n",
            "       |   Unzipping corpora/toolbox.zip.\n",
            "       | Downloading package treebank to /root/nltk_data...\n",
            "       |   Unzipping corpora/treebank.zip.\n",
            "       | Downloading package twitter_samples to /root/nltk_data...\n",
            "       |   Unzipping corpora/twitter_samples.zip.\n",
            "       | Downloading package udhr to /root/nltk_data...\n",
            "       |   Unzipping corpora/udhr.zip.\n",
            "       | Downloading package udhr2 to /root/nltk_data...\n",
            "       |   Unzipping corpora/udhr2.zip.\n",
            "       | Downloading package unicode_samples to /root/nltk_data...\n",
            "       |   Unzipping corpora/unicode_samples.zip.\n",
            "       | Downloading package universal_treebanks_v20 to\n",
            "       |     /root/nltk_data...\n",
            "       | Downloading package verbnet to /root/nltk_data...\n",
            "       |   Unzipping corpora/verbnet.zip.\n",
            "       | Downloading package verbnet3 to /root/nltk_data...\n",
            "       |   Unzipping corpora/verbnet3.zip.\n",
            "       | Downloading package webtext to /root/nltk_data...\n",
            "       |   Unzipping corpora/webtext.zip.\n",
            "       | Downloading package wordnet to /root/nltk_data...\n",
            "       |   Unzipping corpora/wordnet.zip.\n",
            "       | Downloading package wordnet_ic to /root/nltk_data...\n",
            "       |   Unzipping corpora/wordnet_ic.zip.\n",
            "       | Downloading package words to /root/nltk_data...\n",
            "       |   Unzipping corpora/words.zip.\n",
            "       | Downloading package ycoe to /root/nltk_data...\n",
            "       |   Unzipping corpora/ycoe.zip.\n",
            "       | Downloading package rslp to /root/nltk_data...\n",
            "       |   Unzipping stemmers/rslp.zip.\n",
            "       | Downloading package maxent_treebank_pos_tagger to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "       | Downloading package universal_tagset to /root/nltk_data...\n",
            "       |   Unzipping taggers/universal_tagset.zip.\n",
            "       | Downloading package maxent_ne_chunker to /root/nltk_data...\n",
            "       |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "       | Downloading package punkt to /root/nltk_data...\n",
            "       |   Unzipping tokenizers/punkt.zip.\n",
            "       | Downloading package book_grammars to /root/nltk_data...\n",
            "       |   Unzipping grammars/book_grammars.zip.\n",
            "       | Downloading package sample_grammars to /root/nltk_data...\n",
            "       |   Unzipping grammars/sample_grammars.zip.\n",
            "       | Downloading package spanish_grammars to /root/nltk_data...\n",
            "       |   Unzipping grammars/spanish_grammars.zip.\n",
            "       | Downloading package basque_grammars to /root/nltk_data...\n",
            "       |   Unzipping grammars/basque_grammars.zip.\n",
            "       | Downloading package large_grammars to /root/nltk_data...\n",
            "       |   Unzipping grammars/large_grammars.zip.\n",
            "       | Downloading package tagsets to /root/nltk_data...\n",
            "       |   Unzipping help/tagsets.zip.\n",
            "       | Downloading package snowball_data to /root/nltk_data...\n",
            "       | Downloading package bllip_wsj_no_aux to /root/nltk_data...\n",
            "       |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "       | Downloading package word2vec_sample to /root/nltk_data...\n",
            "       |   Unzipping models/word2vec_sample.zip.\n",
            "       | Downloading package panlex_swadesh to /root/nltk_data...\n",
            "       | Downloading package mte_teip5 to /root/nltk_data...\n",
            "       |   Unzipping corpora/mte_teip5.zip.\n",
            "       | Downloading package averaged_perceptron_tagger to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "       | Downloading package averaged_perceptron_tagger_ru to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping taggers/averaged_perceptron_tagger_ru.zip.\n",
            "       | Downloading package perluniprops to /root/nltk_data...\n",
            "       |   Unzipping misc/perluniprops.zip.\n",
            "       | Downloading package nonbreaking_prefixes to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "       | Downloading package vader_lexicon to /root/nltk_data...\n",
            "       | Downloading package porter_test to /root/nltk_data...\n",
            "       |   Unzipping stemmers/porter_test.zip.\n",
            "       | Downloading package wmt15_eval to /root/nltk_data...\n",
            "       |   Unzipping models/wmt15_eval.zip.\n",
            "       | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "       |   Unzipping misc/mwa_ppdb.zip.\n",
            "       | \n",
            "     Done downloading collection all\n",
            "\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> q\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DVtFIVAqqpy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 857
        },
        "outputId": "17356786-8328-4aa7-e6ea-517b886a0984"
      },
      "source": [
        "# Task 2 - IMPORT BROWN CORPUS AND ACCESSING DATA\n",
        "\n",
        "from nltk.corpus import brown\n",
        "brown.categories()\n",
        "brown.words(categories='adventure')[:50]"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Dan',\n",
              " 'Morgan',\n",
              " 'told',\n",
              " 'himself',\n",
              " 'he',\n",
              " 'would',\n",
              " 'forget',\n",
              " 'Ann',\n",
              " 'Turner',\n",
              " '.',\n",
              " 'He',\n",
              " 'was',\n",
              " 'well',\n",
              " 'rid',\n",
              " 'of',\n",
              " 'her',\n",
              " '.',\n",
              " 'He',\n",
              " 'certainly',\n",
              " \"didn't\",\n",
              " 'want',\n",
              " 'a',\n",
              " 'wife',\n",
              " 'who',\n",
              " 'was',\n",
              " 'fickle',\n",
              " 'as',\n",
              " 'Ann',\n",
              " '.',\n",
              " 'If',\n",
              " 'he',\n",
              " 'had',\n",
              " 'married',\n",
              " 'her',\n",
              " ',',\n",
              " \"he'd\",\n",
              " 'have',\n",
              " 'been',\n",
              " 'asking',\n",
              " 'for',\n",
              " 'trouble',\n",
              " '.',\n",
              " 'But',\n",
              " 'all',\n",
              " 'of',\n",
              " 'this',\n",
              " 'was',\n",
              " 'rationalization',\n",
              " '.',\n",
              " 'Sometimes']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uN3gNzGVsjPn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 857
        },
        "outputId": "a7bbbf34-51e8-4e86-a7c5-83eacba9a82d"
      },
      "source": [
        "# Task 3 - IMPORT INAUGRAL CORPUS AND ACCESS DATA\n",
        "#includes every president's inaugral address from 1789 to 2017\n",
        "from nltk.corpus import inaugural\n",
        "inaugural.fileids()\n",
        "# inaugural.words(fileids = '1861-Lincoln.txt')[:50]\n",
        "inaugural.words(fileids = '2017-Trump.txt')[:50]"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Chief',\n",
              " 'Justice',\n",
              " 'Roberts',\n",
              " ',',\n",
              " 'President',\n",
              " 'Carter',\n",
              " ',',\n",
              " 'President',\n",
              " 'Clinton',\n",
              " ',',\n",
              " 'President',\n",
              " 'Bush',\n",
              " ',',\n",
              " 'President',\n",
              " 'Obama',\n",
              " ',',\n",
              " 'fellow',\n",
              " 'Americans',\n",
              " ',',\n",
              " 'and',\n",
              " 'people',\n",
              " 'of',\n",
              " 'the',\n",
              " 'world',\n",
              " ':',\n",
              " 'Thank',\n",
              " 'you',\n",
              " '.',\n",
              " 'We',\n",
              " ',',\n",
              " 'the',\n",
              " 'citizens',\n",
              " 'of',\n",
              " 'America',\n",
              " ',',\n",
              " 'are',\n",
              " 'now',\n",
              " 'joined',\n",
              " 'in',\n",
              " 'a',\n",
              " 'great',\n",
              " 'national',\n",
              " 'effort',\n",
              " 'to',\n",
              " 'rebuild',\n",
              " 'our',\n",
              " 'country',\n",
              " 'and',\n",
              " 'restore',\n",
              " 'its']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MvKIcQ4LuXHW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TASK 4 - IMPORTING WEBTEXT CORPUS AND ACCESSING DATA\n",
        "# from nltk.corpus import webtext\n",
        "# webtext.fileids()"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EtEI5y5CwE7y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3a7b6b6b-dcb0-41f4-c120-2734bdc41c52"
      },
      "source": [
        "# TASK 5 - FREQUENCY DISTRIBUTION OF WORDS IN A TEXT\n",
        "text1 = '''Moby-Dick; or, The Whale is an 1851 novel by American writer Herman Melville. The book is the sailor Ishmael's narrative of the obsessive quest of Ahab, captain of the whaling ship Pequod, for revenge on Moby Dick, the giant white sperm whale that on the ship's previous voyage bit off Ahab's leg at the knee. A contribution to the literature of the American Renaissance, Moby-Dick was published to mixed reviews, was a commercial failure, and was out of print at the time of the author's death in 1891. Its reputation as a \"Great American Novel\" was established only in the 20th century, after the centennial of its author's birth. William Faulkner said he wished he had written the book himself,[1] and D. H. Lawrence called it \"one of the strangest and most wonderful books in the world\" and \"the greatest book of the sea ever written\".[2] Its opening sentence, \"Call me Ishmael\", is among world literature's most famous.'''\n",
        "fd = nltk.FreqDist(text1.split())\n",
        "fd"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FreqDist({'\"Call': 1,\n",
              "          '\"Great': 1,\n",
              "          '\"one': 1,\n",
              "          '\"the': 1,\n",
              "          '1851': 1,\n",
              "          '1891.': 1,\n",
              "          '20th': 1,\n",
              "          'A': 1,\n",
              "          \"Ahab's\": 1,\n",
              "          'Ahab,': 1,\n",
              "          'American': 3,\n",
              "          'D.': 1,\n",
              "          'Dick,': 1,\n",
              "          'Faulkner': 1,\n",
              "          'H.': 1,\n",
              "          'Herman': 1,\n",
              "          'Ishmael\",': 1,\n",
              "          \"Ishmael's\": 1,\n",
              "          'Its': 2,\n",
              "          'Lawrence': 1,\n",
              "          'Melville.': 1,\n",
              "          'Moby': 1,\n",
              "          'Moby-Dick': 1,\n",
              "          'Moby-Dick;': 1,\n",
              "          'Novel\"': 1,\n",
              "          'Pequod,': 1,\n",
              "          'Renaissance,': 1,\n",
              "          'The': 2,\n",
              "          'Whale': 1,\n",
              "          'William': 1,\n",
              "          'a': 2,\n",
              "          'after': 1,\n",
              "          'among': 1,\n",
              "          'an': 1,\n",
              "          'and': 4,\n",
              "          'as': 1,\n",
              "          'at': 2,\n",
              "          \"author's\": 2,\n",
              "          'birth.': 1,\n",
              "          'bit': 1,\n",
              "          'book': 3,\n",
              "          'books': 1,\n",
              "          'by': 1,\n",
              "          'called': 1,\n",
              "          'captain': 1,\n",
              "          'centennial': 1,\n",
              "          'century,': 1,\n",
              "          'commercial': 1,\n",
              "          'contribution': 1,\n",
              "          'death': 1,\n",
              "          'established': 1,\n",
              "          'ever': 1,\n",
              "          'failure,': 1,\n",
              "          'famous.': 1,\n",
              "          'for': 1,\n",
              "          'giant': 1,\n",
              "          'greatest': 1,\n",
              "          'had': 1,\n",
              "          'he': 2,\n",
              "          'himself,[1]': 1,\n",
              "          'in': 3,\n",
              "          'is': 3,\n",
              "          'it': 1,\n",
              "          'its': 1,\n",
              "          'knee.': 1,\n",
              "          'leg': 1,\n",
              "          'literature': 1,\n",
              "          \"literature's\": 1,\n",
              "          'me': 1,\n",
              "          'mixed': 1,\n",
              "          'most': 2,\n",
              "          'narrative': 1,\n",
              "          'novel': 1,\n",
              "          'obsessive': 1,\n",
              "          'of': 9,\n",
              "          'off': 1,\n",
              "          'on': 2,\n",
              "          'only': 1,\n",
              "          'opening': 1,\n",
              "          'or,': 1,\n",
              "          'out': 1,\n",
              "          'previous': 1,\n",
              "          'print': 1,\n",
              "          'published': 1,\n",
              "          'quest': 1,\n",
              "          'reputation': 1,\n",
              "          'revenge': 1,\n",
              "          'reviews,': 1,\n",
              "          'said': 1,\n",
              "          'sailor': 1,\n",
              "          'sea': 1,\n",
              "          'sentence,': 1,\n",
              "          'ship': 1,\n",
              "          \"ship's\": 1,\n",
              "          'sperm': 1,\n",
              "          'strangest': 1,\n",
              "          'that': 1,\n",
              "          'the': 16,\n",
              "          'time': 1,\n",
              "          'to': 2,\n",
              "          'voyage': 1,\n",
              "          'was': 4,\n",
              "          'whale': 1,\n",
              "          'whaling': 1,\n",
              "          'white': 1,\n",
              "          'wished': 1,\n",
              "          'wonderful': 1,\n",
              "          'world': 1,\n",
              "          'world\"': 1,\n",
              "          'writer': 1,\n",
              "          'written': 1,\n",
              "          'written\".[2]': 1})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PsW8xYKTxz4Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "outputId": "ac8b8fa9-a01a-4c04-db57-ff2601895a57"
      },
      "source": [
        "# TASK 6 - CONSITIONAL FREQUENCY DISTRIBUTION OF WORDS IN A TEXT\n",
        "from nltk.probability import ConditionalFreqDist\n",
        "cfd = ConditionalFreqDist((len(word),word) for word in text1.split())\n",
        "cfd[5]"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FreqDist({'\"Call': 1,\n",
              "          '1891.': 1,\n",
              "          'Ahab,': 1,\n",
              "          'Dick,': 1,\n",
              "          'Whale': 1,\n",
              "          'after': 1,\n",
              "          'among': 1,\n",
              "          'books': 1,\n",
              "          'death': 1,\n",
              "          'giant': 1,\n",
              "          'knee.': 1,\n",
              "          'mixed': 1,\n",
              "          'novel': 1,\n",
              "          'print': 1,\n",
              "          'quest': 1,\n",
              "          'sperm': 1,\n",
              "          'whale': 1,\n",
              "          'white': 1,\n",
              "          'world': 1})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6e53fjWu0YMM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# HW 1: To determine Frequency Distribution and Conditional Frequency Distribution of any one of the Presidential inaugural addresses\n",
        "# Word CLoud --> wordle.net\n"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HqUYoPPx1sZe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "11d00368-27dc-4586-d3d1-d027fec191a8"
      },
      "source": [
        "text2 = inaugural.words(fileids = '2009-Obama.txt')\n",
        "text2"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['My', 'fellow', 'citizens', ':', 'I', 'stand', 'here', ...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7Y0OdbG1-Nn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5a6485b5-2385-4f4f-8cdb-0e40f73dc47a"
      },
      "source": [
        "fd = nltk.FreqDist(text1.split())\n",
        "fd"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FreqDist({'\"Call': 1,\n",
              "          '\"Great': 1,\n",
              "          '\"one': 1,\n",
              "          '\"the': 1,\n",
              "          '1851': 1,\n",
              "          '1891.': 1,\n",
              "          '20th': 1,\n",
              "          'A': 1,\n",
              "          \"Ahab's\": 1,\n",
              "          'Ahab,': 1,\n",
              "          'American': 3,\n",
              "          'D.': 1,\n",
              "          'Dick,': 1,\n",
              "          'Faulkner': 1,\n",
              "          'H.': 1,\n",
              "          'Herman': 1,\n",
              "          'Ishmael\",': 1,\n",
              "          \"Ishmael's\": 1,\n",
              "          'Its': 2,\n",
              "          'Lawrence': 1,\n",
              "          'Melville.': 1,\n",
              "          'Moby': 1,\n",
              "          'Moby-Dick': 1,\n",
              "          'Moby-Dick;': 1,\n",
              "          'Novel\"': 1,\n",
              "          'Pequod,': 1,\n",
              "          'Renaissance,': 1,\n",
              "          'The': 2,\n",
              "          'Whale': 1,\n",
              "          'William': 1,\n",
              "          'a': 2,\n",
              "          'after': 1,\n",
              "          'among': 1,\n",
              "          'an': 1,\n",
              "          'and': 4,\n",
              "          'as': 1,\n",
              "          'at': 2,\n",
              "          \"author's\": 2,\n",
              "          'birth.': 1,\n",
              "          'bit': 1,\n",
              "          'book': 3,\n",
              "          'books': 1,\n",
              "          'by': 1,\n",
              "          'called': 1,\n",
              "          'captain': 1,\n",
              "          'centennial': 1,\n",
              "          'century,': 1,\n",
              "          'commercial': 1,\n",
              "          'contribution': 1,\n",
              "          'death': 1,\n",
              "          'established': 1,\n",
              "          'ever': 1,\n",
              "          'failure,': 1,\n",
              "          'famous.': 1,\n",
              "          'for': 1,\n",
              "          'giant': 1,\n",
              "          'greatest': 1,\n",
              "          'had': 1,\n",
              "          'he': 2,\n",
              "          'himself,[1]': 1,\n",
              "          'in': 3,\n",
              "          'is': 3,\n",
              "          'it': 1,\n",
              "          'its': 1,\n",
              "          'knee.': 1,\n",
              "          'leg': 1,\n",
              "          'literature': 1,\n",
              "          \"literature's\": 1,\n",
              "          'me': 1,\n",
              "          'mixed': 1,\n",
              "          'most': 2,\n",
              "          'narrative': 1,\n",
              "          'novel': 1,\n",
              "          'obsessive': 1,\n",
              "          'of': 9,\n",
              "          'off': 1,\n",
              "          'on': 2,\n",
              "          'only': 1,\n",
              "          'opening': 1,\n",
              "          'or,': 1,\n",
              "          'out': 1,\n",
              "          'previous': 1,\n",
              "          'print': 1,\n",
              "          'published': 1,\n",
              "          'quest': 1,\n",
              "          'reputation': 1,\n",
              "          'revenge': 1,\n",
              "          'reviews,': 1,\n",
              "          'said': 1,\n",
              "          'sailor': 1,\n",
              "          'sea': 1,\n",
              "          'sentence,': 1,\n",
              "          'ship': 1,\n",
              "          \"ship's\": 1,\n",
              "          'sperm': 1,\n",
              "          'strangest': 1,\n",
              "          'that': 1,\n",
              "          'the': 16,\n",
              "          'time': 1,\n",
              "          'to': 2,\n",
              "          'voyage': 1,\n",
              "          'was': 4,\n",
              "          'whale': 1,\n",
              "          'whaling': 1,\n",
              "          'white': 1,\n",
              "          'wished': 1,\n",
              "          'wonderful': 1,\n",
              "          'world': 1,\n",
              "          'world\"': 1,\n",
              "          'writer': 1,\n",
              "          'written': 1,\n",
              "          'written\".[2]': 1})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Es7Zf-ef2Kik",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9946a70b-d1e7-4967-a5bf-ca31add0e851"
      },
      "source": [
        "from nltk.probability import ConditionalFreqDist\n",
        "cfd = nltk.ConditionalFreqDist((len(word),word) for word in text2)\n",
        "cfd"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ConditionalFreqDist(nltk.probability.FreqDist,\n",
              "                    {1: FreqDist({'!': 1,\n",
              "                               '\"': 1,\n",
              "                               \"'\": 13,\n",
              "                               ',': 130,\n",
              "                               '-': 9,\n",
              "                               '.': 108,\n",
              "                               ':': 4,\n",
              "                               ';': 22,\n",
              "                               'I': 3,\n",
              "                               'a': 47,\n",
              "                               's': 12}),\n",
              "                     2: FreqDist({'--': 22,\n",
              "                               '.\"': 1,\n",
              "                               'As': 2,\n",
              "                               'At': 2,\n",
              "                               'In': 3,\n",
              "                               'It': 4,\n",
              "                               'My': 1,\n",
              "                               'On': 2,\n",
              "                               'So': 3,\n",
              "                               'To': 4,\n",
              "                               'We': 12,\n",
              "                               'an': 3,\n",
              "                               'as': 9,\n",
              "                               'at': 6,\n",
              "                               'be': 12,\n",
              "                               'by': 8,\n",
              "                               'do': 7,\n",
              "                               'he': 1,\n",
              "                               'if': 1,\n",
              "                               'in': 22,\n",
              "                               'is': 36,\n",
              "                               'it': 15,\n",
              "                               'my': 1,\n",
              "                               'no': 7,\n",
              "                               'of': 82,\n",
              "                               'on': 15,\n",
              "                               'or': 11,\n",
              "                               'so': 6,\n",
              "                               'to': 66,\n",
              "                               'up': 4,\n",
              "                               'us': 23,\n",
              "                               'we': 50}),\n",
              "                     3: FreqDist({'...': 3,\n",
              "                               'All': 2,\n",
              "                               'And': 6,\n",
              "                               'But': 3,\n",
              "                               'For': 8,\n",
              "                               'God': 5,\n",
              "                               'Its': 1,\n",
              "                               'Khe': 1,\n",
              "                               'Let': 2,\n",
              "                               'Nor': 1,\n",
              "                               'Now': 1,\n",
              "                               'Our': 9,\n",
              "                               'The': 9,\n",
              "                               'Yet': 1,\n",
              "                               'act': 1,\n",
              "                               'age': 2,\n",
              "                               'ago': 1,\n",
              "                               'all': 7,\n",
              "                               'and': 105,\n",
              "                               'are': 22,\n",
              "                               'ask': 1,\n",
              "                               'bad': 1,\n",
              "                               'big': 2,\n",
              "                               'but': 17,\n",
              "                               'can': 13,\n",
              "                               'cut': 1,\n",
              "                               'day': 5,\n",
              "                               'did': 2,\n",
              "                               'end': 4,\n",
              "                               'era': 2,\n",
              "                               'eye': 1,\n",
              "                               'far': 4,\n",
              "                               'few': 1,\n",
              "                               'for': 15,\n",
              "                               'has': 14,\n",
              "                               'his': 1,\n",
              "                               'how': 1,\n",
              "                               'icy': 2,\n",
              "                               'ill': 1,\n",
              "                               'its': 7,\n",
              "                               'job': 1,\n",
              "                               'law': 1,\n",
              "                               'lay': 1,\n",
              "                               'let': 5,\n",
              "                               'lie': 1,\n",
              "                               'man': 3,\n",
              "                               'may': 3,\n",
              "                               'men': 4,\n",
              "                               'met': 2,\n",
              "                               'new': 11,\n",
              "                               'non': 1,\n",
              "                               'nor': 4,\n",
              "                               'not': 16,\n",
              "                               'now': 5,\n",
              "                               'off': 3,\n",
              "                               'old': 3,\n",
              "                               'one': 2,\n",
              "                               'our': 58,\n",
              "                               'out': 3,\n",
              "                               'pat': 1,\n",
              "                               'raw': 1,\n",
              "                               'run': 1,\n",
              "                               'saw': 1,\n",
              "                               'say': 3,\n",
              "                               'see': 1,\n",
              "                               'set': 1,\n",
              "                               'sow': 1,\n",
              "                               'sum': 1,\n",
              "                               'sun': 1,\n",
              "                               'the': 126,\n",
              "                               'too': 6,\n",
              "                               'use': 2,\n",
              "                               'war': 2,\n",
              "                               'was': 5,\n",
              "                               'way': 2,\n",
              "                               'who': 14,\n",
              "                               'why': 2,\n",
              "                               'yes': 1,\n",
              "                               'yet': 1,\n",
              "                               'you': 14}),\n",
              "                     4: FreqDist({'Bush': 1,\n",
              "                               'Iraq': 1,\n",
              "                               'Jews': 1,\n",
              "                               'Less': 1,\n",
              "                               'Sahn': 1,\n",
              "                               'That': 1,\n",
              "                               'They': 6,\n",
              "                               'This': 4,\n",
              "                               'Time': 1,\n",
              "                               'West': 2,\n",
              "                               'What': 3,\n",
              "                               'With': 2,\n",
              "                               'ages': 1,\n",
              "                               'aims': 1,\n",
              "                               'also': 2,\n",
              "                               'back': 2,\n",
              "                               'band': 1,\n",
              "                               'been': 8,\n",
              "                               'bind': 1,\n",
              "                               'bold': 1,\n",
              "                               'born': 1,\n",
              "                               'came': 1,\n",
              "                               'care': 3,\n",
              "                               'cars': 1,\n",
              "                               'city': 1,\n",
              "                               'come': 4,\n",
              "                               'cost': 1,\n",
              "                               'dark': 1,\n",
              "                               'data': 1,\n",
              "                               'died': 1,\n",
              "                               'does': 1,\n",
              "                               'done': 2,\n",
              "                               'down': 1,\n",
              "                               'dust': 1,\n",
              "                               'each': 2,\n",
              "                               'even': 2,\n",
              "                               'eyes': 1,\n",
              "                               'face': 2,\n",
              "                               'fail': 2,\n",
              "                               'fair': 1,\n",
              "                               'fame': 1,\n",
              "                               'fate': 1,\n",
              "                               'fear': 2,\n",
              "                               'feed': 2,\n",
              "                               'find': 2,\n",
              "                               'firm': 1,\n",
              "                               'fist': 1,\n",
              "                               'flow': 1,\n",
              "                               'foes': 1,\n",
              "                               'four': 1,\n",
              "                               'free': 2,\n",
              "                               'from': 5,\n",
              "                               'fuel': 1,\n",
              "                               'full': 1,\n",
              "                               'gift': 2,\n",
              "                               'give': 1,\n",
              "                               'good': 2,\n",
              "                               'hand': 1,\n",
              "                               'hard': 4,\n",
              "                               'have': 16,\n",
              "                               'held': 1,\n",
              "                               'help': 1,\n",
              "                               'here': 1,\n",
              "                               'high': 1,\n",
              "                               'hope': 3,\n",
              "                               'hour': 1,\n",
              "                               'idea': 1,\n",
              "                               'ills': 1,\n",
              "                               'jobs': 3,\n",
              "                               'join': 1,\n",
              "                               'just': 3,\n",
              "                               'knew': 1,\n",
              "                               'know': 5,\n",
              "                               'land': 1,\n",
              "                               'lash': 1,\n",
              "                               'last': 3,\n",
              "                               'lead': 1,\n",
              "                               'less': 6,\n",
              "                               'life': 3,\n",
              "                               'like': 2,\n",
              "                               'live': 1,\n",
              "                               'long': 4,\n",
              "                               'look': 1,\n",
              "                               'lose': 1,\n",
              "                               'lost': 1,\n",
              "                               'make': 2,\n",
              "                               'mall': 1,\n",
              "                               'many': 3,\n",
              "                               'mark': 1,\n",
              "                               'meet': 4,\n",
              "                               'more': 5,\n",
              "                               'most': 3,\n",
              "                               'move': 1,\n",
              "                               'much': 1,\n",
              "                               'must': 8,\n",
              "                               'next': 1,\n",
              "                               'oath': 3,\n",
              "                               'once': 3,\n",
              "                               'only': 5,\n",
              "                               'ours': 1,\n",
              "                               'over': 3,\n",
              "                               'part': 2,\n",
              "                               'pass': 1,\n",
              "                               'path': 2,\n",
              "                               'pick': 1,\n",
              "                               'play': 2,\n",
              "                               'poor': 1,\n",
              "                               'race': 1,\n",
              "                               'read': 1,\n",
              "                               'real': 1,\n",
              "                               'risk': 1,\n",
              "                               'road': 1,\n",
              "                               'role': 1,\n",
              "                               'roll': 1,\n",
              "                               'rule': 1,\n",
              "                               'said': 1,\n",
              "                               'sake': 1,\n",
              "                               'seek': 4,\n",
              "                               'sees': 1,\n",
              "                               'shed': 1,\n",
              "                               'side': 1,\n",
              "                               'size': 1,\n",
              "                               'snow': 1,\n",
              "                               'soil': 1,\n",
              "                               'some': 3,\n",
              "                               'soon': 1,\n",
              "                               'span': 1,\n",
              "                               'spin': 1,\n",
              "                               'take': 2,\n",
              "                               'task': 2,\n",
              "                               'tell': 1,\n",
              "                               'than': 8,\n",
              "                               'that': 48,\n",
              "                               'them': 4,\n",
              "                               'then': 2,\n",
              "                               'they': 11,\n",
              "                               'this': 20,\n",
              "                               'till': 1,\n",
              "                               'time': 5,\n",
              "                               'told': 1,\n",
              "                               'true': 2,\n",
              "                               'turn': 1,\n",
              "                               'upon': 3,\n",
              "                               'very': 1,\n",
              "                               'wage': 1,\n",
              "                               'ways': 1,\n",
              "                               'week': 1,\n",
              "                               'well': 2,\n",
              "                               'were': 3,\n",
              "                               'what': 5,\n",
              "                               'when': 7,\n",
              "                               'whip': 1,\n",
              "                               'will': 19,\n",
              "                               'with': 11,\n",
              "                               'work': 6,\n",
              "                               'worn': 1,\n",
              "                               'year': 2,\n",
              "                               'your': 3}),\n",
              "                     5: FreqDist({'Earth': 2,\n",
              "                               'Forty': 1,\n",
              "                               'Gross': 1,\n",
              "                               'Homes': 1,\n",
              "                               'Thank': 1,\n",
              "                               'Their': 1,\n",
              "                               'These': 2,\n",
              "                               'Those': 1,\n",
              "                               'Today': 1,\n",
              "                               'Where': 2,\n",
              "                               'again': 2,\n",
              "                               'alone': 1,\n",
              "                               'apply': 1,\n",
              "                               'aside': 1,\n",
              "                               'badly': 1,\n",
              "                               'based': 1,\n",
              "                               'began': 1,\n",
              "                               'begin': 2,\n",
              "                               'birth': 2,\n",
              "                               'blame': 1,\n",
              "                               'bless': 2,\n",
              "                               'blood': 2,\n",
              "                               'borne': 1,\n",
              "                               'brave': 2,\n",
              "                               'break': 1,\n",
              "                               'build': 2,\n",
              "                               'calls': 2,\n",
              "                               'carry': 1,\n",
              "                               'cause': 1,\n",
              "                               'child': 2,\n",
              "                               'civil': 1,\n",
              "                               'clean': 1,\n",
              "                               'cling': 1,\n",
              "                               'could': 1,\n",
              "                               'creed': 1,\n",
              "                               'depth': 1,\n",
              "                               'doers': 1,\n",
              "                               'doubt': 1,\n",
              "                               'drawn': 1,\n",
              "                               'dying': 1,\n",
              "                               'earth': 1,\n",
              "                               'enemy': 1,\n",
              "                               'enjoy': 1,\n",
              "                               'equal': 1,\n",
              "                               'every': 8,\n",
              "                               'faced': 2,\n",
              "                               'faint': 1,\n",
              "                               'faith': 2,\n",
              "                               'false': 2,\n",
              "                               'farms': 1,\n",
              "                               'fixed': 1,\n",
              "                               'force': 3,\n",
              "                               'forge': 1,\n",
              "                               'forth': 2,\n",
              "                               'given': 2,\n",
              "                               'globe': 1,\n",
              "                               'goods': 1,\n",
              "                               'grace': 1,\n",
              "                               'great': 1,\n",
              "                               'greed': 1,\n",
              "                               'grids': 1,\n",
              "                               'grows': 2,\n",
              "                               'hands': 1,\n",
              "                               'heart': 1,\n",
              "                               'helps': 1,\n",
              "                               'honor': 1,\n",
              "                               'hours': 2,\n",
              "                               'judge': 1,\n",
              "                               'labor': 1,\n",
              "                               'leave': 1,\n",
              "                               'light': 2,\n",
              "                               'lines': 2,\n",
              "                               'local': 1,\n",
              "                               'lower': 2,\n",
              "                               'midst': 1,\n",
              "                               'might': 2,\n",
              "                               'minds': 2,\n",
              "                               'month': 1,\n",
              "                               'never': 2,\n",
              "                               'noble': 1,\n",
              "                               'often': 2,\n",
              "                               'other': 1,\n",
              "                               'peace': 4,\n",
              "                               'petty': 1,\n",
              "                               'place': 1,\n",
              "                               'plans': 1,\n",
              "                               'power': 4,\n",
              "                               'price': 1,\n",
              "                               'quiet': 1,\n",
              "                               'raise': 1,\n",
              "                               'reach': 1,\n",
              "                               'ready': 1,\n",
              "                               'river': 1,\n",
              "                               'roads': 1,\n",
              "                               'route': 1,\n",
              "                               'scale': 1,\n",
              "                               'seeks': 1,\n",
              "                               'seize': 1,\n",
              "                               'shall': 3,\n",
              "                               'shape': 1,\n",
              "                               'short': 2,\n",
              "                               'shown': 1,\n",
              "                               'sixty': 1,\n",
              "                               'skill': 1,\n",
              "                               'small': 3,\n",
              "                               'smoke': 1,\n",
              "                               'spend': 1,\n",
              "                               'stale': 1,\n",
              "                               'stand': 2,\n",
              "                               'state': 1,\n",
              "                               'still': 2,\n",
              "                               'storm': 1,\n",
              "                               'swift': 1,\n",
              "                               'swill': 1,\n",
              "                               'taken': 2,\n",
              "                               'tanks': 1,\n",
              "                               'thank': 1,\n",
              "                               'their': 9,\n",
              "                               'there': 3,\n",
              "                               'these': 7,\n",
              "                               'those': 10,\n",
              "                               'tides': 1,\n",
              "                               'today': 5,\n",
              "                               'tribe': 1,\n",
              "                               'trust': 2,\n",
              "                               'unity': 1,\n",
              "                               'vital': 1,\n",
              "                               'waver': 1,\n",
              "                               'where': 1,\n",
              "                               'which': 4,\n",
              "                               'whose': 1,\n",
              "                               'wield': 1,\n",
              "                               'winds': 1,\n",
              "                               'woman': 1,\n",
              "                               'women': 4,\n",
              "                               'words': 4,\n",
              "                               'works': 1,\n",
              "                               'world': 7,\n",
              "                               'would': 1,\n",
              "                               'wrong': 1,\n",
              "                               'years': 1,\n",
              "                               'young': 1}),\n",
              "                     6: FreqDist({'Guided': 1,\n",
              "                               'Hindus': 1,\n",
              "                               'Muslim': 1,\n",
              "                               'People': 1,\n",
              "                               'Rather': 1,\n",
              "                               'Recall': 1,\n",
              "                               'States': 1,\n",
              "                               'United': 1,\n",
              "                               'accept': 1,\n",
              "                               'across': 3,\n",
              "                               'action': 1,\n",
              "                               'afford': 2,\n",
              "                               'always': 1,\n",
              "                               'amidst': 1,\n",
              "                               'answer': 2,\n",
              "                               'around': 1,\n",
              "                               'assure': 1,\n",
              "                               'before': 4,\n",
              "                               'better': 2,\n",
              "                               'bigger': 1,\n",
              "                               'bitter': 1,\n",
              "                               'bodies': 1,\n",
              "                               'brings': 1,\n",
              "                               'broken': 1,\n",
              "                               'cannot': 6,\n",
              "                               'chance': 1,\n",
              "                               'change': 1,\n",
              "                               'choice': 1,\n",
              "                               'choose': 1,\n",
              "                               'chosen': 1,\n",
              "                               'clouds': 1,\n",
              "                               'common': 6,\n",
              "                               'costly': 1,\n",
              "                               'create': 1,\n",
              "                               'crisis': 4,\n",
              "                               'cynics': 1,\n",
              "                               'danger': 1,\n",
              "                               'deceit': 1,\n",
              "                               'decent': 1,\n",
              "                               'defeat': 1,\n",
              "                               'define': 1,\n",
              "                               'demand': 1,\n",
              "                               'dogmas': 1,\n",
              "                               'during': 1,\n",
              "                               'duties': 2,\n",
              "                               'earned': 2,\n",
              "                               'easily': 1,\n",
              "                               'effect': 1,\n",
              "                               'effort': 1,\n",
              "                               'embody': 1,\n",
              "                               'endure': 1,\n",
              "                               'energy': 1,\n",
              "                               'expand': 1,\n",
              "                               'extend': 2,\n",
              "                               'fallen': 1,\n",
              "                               'falter': 1,\n",
              "                               'father': 3,\n",
              "                               'favors': 1,\n",
              "                               'fellow': 1,\n",
              "                               'filled': 1,\n",
              "                               'former': 1,\n",
              "                               'fought': 1,\n",
              "                               'friend': 2,\n",
              "                               'future': 3,\n",
              "                               'gather': 1,\n",
              "                               'giving': 1,\n",
              "                               'gladly': 1,\n",
              "                               'ground': 1,\n",
              "                               'growth': 1,\n",
              "                               'habits': 1,\n",
              "                               'hatred': 1,\n",
              "                               'health': 2,\n",
              "                               'heroes': 1,\n",
              "                               'humble': 1,\n",
              "                               'hungry': 1,\n",
              "                               'ideals': 3,\n",
              "                               'intend': 1,\n",
              "                               'itself': 1,\n",
              "                               'joined': 1,\n",
              "                               'legacy': 1,\n",
              "                               'lessen': 1,\n",
              "                               'levees': 1,\n",
              "                               'longer': 2,\n",
              "                               'makers': 1,\n",
              "                               'manage': 1,\n",
              "                               'market': 2,\n",
              "                               'moment': 3,\n",
              "                               'months': 1,\n",
              "                               'mutual': 2,\n",
              "                               'narrow': 1,\n",
              "                               'nation': 12,\n",
              "                               'needed': 1,\n",
              "                               'oceans': 1,\n",
              "                               'office': 1,\n",
              "                               'packed': 1,\n",
              "                               'parent': 1,\n",
              "                               'passed': 2,\n",
              "                               'patrol': 1,\n",
              "                               'people': 6,\n",
              "                               'perils': 1,\n",
              "                               'places': 1,\n",
              "                               'planet': 2,\n",
              "                               'please': 1,\n",
              "                               'pledge': 1,\n",
              "                               'plenty': 1,\n",
              "                               'plowed': 1,\n",
              "                               'prefer': 1,\n",
              "                               'public': 1,\n",
              "                               'pursue': 1,\n",
              "                               'raging': 1,\n",
              "                               'rather': 2,\n",
              "                               'reform': 1,\n",
              "                               'regard': 1,\n",
              "                               'reject': 1,\n",
              "                               'relies': 1,\n",
              "                               'remain': 2,\n",
              "                               'return': 1,\n",
              "                               'reveal': 1,\n",
              "                               'riches': 1,\n",
              "                               'rights': 1,\n",
              "                               'rising': 1,\n",
              "                               'rugged': 1,\n",
              "                               'sacred': 1,\n",
              "                               'safely': 1,\n",
              "                               'safety': 1,\n",
              "                               'search': 1,\n",
              "                               'served': 1,\n",
              "                               'shaped': 1,\n",
              "                               'shores': 1,\n",
              "                               'sights': 1,\n",
              "                               'simply': 1,\n",
              "                               'source': 1,\n",
              "                               'spirit': 5,\n",
              "                               'spoken': 1,\n",
              "                               'storms': 2,\n",
              "                               'sturdy': 1,\n",
              "                               'surely': 1,\n",
              "                               'surest': 1,\n",
              "                               'system': 1,\n",
              "                               'takers': 1,\n",
              "                               'tasted': 1,\n",
              "                               'terror': 1,\n",
              "                               'tested': 1,\n",
              "                               'things': 4,\n",
              "                               'threat': 1,\n",
              "                               'toiled': 1,\n",
              "                               'truths': 1,\n",
              "                               'united': 1,\n",
              "                               'values': 1,\n",
              "                               'virtue': 2,\n",
              "                               'vision': 1,\n",
              "                               'waters': 2,\n",
              "                               'wealth': 2,\n",
              "                               'winter': 2,\n",
              "                               'wisely': 1,\n",
              "                               'worked': 1}),\n",
              "                     7: FreqDist({'America': 10,\n",
              "                               'Concord': 1,\n",
              "                               'Fathers': 1,\n",
              "                               'Instead': 1,\n",
              "                               'Muslims': 1,\n",
              "                               'Product': 1,\n",
              "                               'ability': 1,\n",
              "                               'account': 1,\n",
              "                               'achieve': 1,\n",
              "                               'advance': 1,\n",
              "                               'against': 1,\n",
              "                               'alarmed': 1,\n",
              "                               'already': 1,\n",
              "                               'because': 8,\n",
              "                               'believe': 1,\n",
              "                               'beneath': 1,\n",
              "                               'between': 3,\n",
              "                               'borders': 1,\n",
              "                               'bridges': 1,\n",
              "                               'capital': 1,\n",
              "                               'carried': 3,\n",
              "                               'changed': 1,\n",
              "                               'chapter': 1,\n",
              "                               'charity': 1,\n",
              "                               'charter': 2,\n",
              "                               'choices': 1,\n",
              "                               'coldest': 1,\n",
              "                               'consume': 1,\n",
              "                               'control': 1,\n",
              "                               'country': 2,\n",
              "                               'courage': 3,\n",
              "                               'culture': 1,\n",
              "                               'dangers': 1,\n",
              "                               'darkest': 1,\n",
              "                               'decides': 1,\n",
              "                               'decline': 1,\n",
              "                               'defense': 2,\n",
              "                               'demands': 1,\n",
              "                               'depends': 1,\n",
              "                               'deserts': 1,\n",
              "                               'deserve': 1,\n",
              "                               'destiny': 1,\n",
              "                               'destroy': 1,\n",
              "                               'digital': 1,\n",
              "                               'dignity': 1,\n",
              "                               'discord': 1,\n",
              "                               'dissent': 1,\n",
              "                               'distant': 1,\n",
              "                               'dollars': 1,\n",
              "                               'drafted': 1,\n",
              "                               'earlier': 1,\n",
              "                               'economy': 3,\n",
              "                               'emerged': 1,\n",
              "                               'endured': 1,\n",
              "                               'entitle': 1,\n",
              "                               'example': 1,\n",
              "                               'faction': 1,\n",
              "                               'failure': 1,\n",
              "                               'fascism': 1,\n",
              "                               'finally': 1,\n",
              "                               'forward': 3,\n",
              "                               'freedom': 3,\n",
              "                               'friends': 1,\n",
              "                               'further': 1,\n",
              "                               'greater': 4,\n",
              "                               'harness': 1,\n",
              "                               'hatreds': 1,\n",
              "                               'hearted': 1,\n",
              "                               'history': 3,\n",
              "                               'honesty': 1,\n",
              "                               'horizon': 1,\n",
              "                               'huddled': 1,\n",
              "                               'humbled': 1,\n",
              "                               'imagine': 1,\n",
              "                               'inhabit': 1,\n",
              "                               'journey': 3,\n",
              "                               'keepers': 1,\n",
              "                               'leaders': 1,\n",
              "                               'leisure': 1,\n",
              "                               'liberty': 2,\n",
              "                               'loyalty': 1,\n",
              "                               'meaning': 2,\n",
              "                               'measure': 1,\n",
              "                               'mindful': 1,\n",
              "                               'moments': 1,\n",
              "                               'nagging': 1,\n",
              "                               'nations': 3,\n",
              "                               'network': 1,\n",
              "                               'nothing': 2,\n",
              "                               'nourish': 1,\n",
              "                               'nuclear': 1,\n",
              "                               'nurture': 1,\n",
              "                               'obscure': 1,\n",
              "                               'ordered': 1,\n",
              "                               'outcome': 1,\n",
              "                               'outlast': 1,\n",
              "                               'outside': 1,\n",
              "                               'peoples': 1,\n",
              "                               'prepare': 1,\n",
              "                               'promise': 2,\n",
              "                               'prosper': 1,\n",
              "                               'protect': 1,\n",
              "                               'prudent': 1,\n",
              "                               'purpose': 2,\n",
              "                               'putting': 1,\n",
              "                               'quality': 1,\n",
              "                               'refused': 1,\n",
              "                               'remains': 1,\n",
              "                               'respect': 1,\n",
              "                               'restore': 2,\n",
              "                               'sapping': 1,\n",
              "                               'schools': 2,\n",
              "                               'science': 1,\n",
              "                               'serious': 1,\n",
              "                               'service': 2,\n",
              "                               'settled': 1,\n",
              "                               'shifted': 1,\n",
              "                               'smaller': 1,\n",
              "                               'society': 1,\n",
              "                               'someday': 1,\n",
              "                               'specter': 1,\n",
              "                               'stained': 1,\n",
              "                               'starved': 1,\n",
              "                               'subject': 1,\n",
              "                               'success': 2,\n",
              "                               'suggest': 1,\n",
              "                               'survive': 1,\n",
              "                               'threats': 1,\n",
              "                               'through': 4,\n",
              "                               'towards': 1,\n",
              "                               'unfolds': 1,\n",
              "                               'village': 1,\n",
              "                               'warming': 1,\n",
              "                               'whether': 4,\n",
              "                               'whisper': 1,\n",
              "                               'willing': 2,\n",
              "                               'without': 2,\n",
              "                               'wonders': 1,\n",
              "                               'workers': 2,\n",
              "                               'worldly': 1}),\n",
              "                     8: FreqDist({'American': 2,\n",
              "                               'Domestic': 1,\n",
              "                               'Founding': 1,\n",
              "                               'Normandy': 1,\n",
              "                               'Starting': 1,\n",
              "                               'bestowed': 1,\n",
              "                               'business': 1,\n",
              "                               'capacity': 1,\n",
              "                               'capitals': 1,\n",
              "                               'childish': 1,\n",
              "                               'children': 3,\n",
              "                               'citizens': 1,\n",
              "                               'colleges': 1,\n",
              "                               'commerce': 1,\n",
              "                               'conflict': 2,\n",
              "                               'consider': 1,\n",
              "                               'consumed': 1,\n",
              "                               'continue': 1,\n",
              "                               'currents': 1,\n",
              "                               'defining': 1,\n",
              "                               'demanded': 1,\n",
              "                               'depended': 1,\n",
              "                               'dissolve': 1,\n",
              "                               'electric': 1,\n",
              "                               'emanates': 1,\n",
              "                               'enduring': 2,\n",
              "                               'evidence': 1,\n",
              "                               'expanded': 1,\n",
              "                               'faithful': 1,\n",
              "                               'families': 1,\n",
              "                               'flourish': 1,\n",
              "                               'founding': 1,\n",
              "                               'generate': 1,\n",
              "                               'grandest': 1,\n",
              "                               'grateful': 1,\n",
              "                               'hardship': 1,\n",
              "                               'heritage': 1,\n",
              "                               'humanity': 1,\n",
              "                               'humility': 1,\n",
              "                               'inducing': 1,\n",
              "                               'interest': 1,\n",
              "                               'justness': 1,\n",
              "                               'kindness': 1,\n",
              "                               'language': 1,\n",
              "                               'memories': 1,\n",
              "                               'missiles': 1,\n",
              "                               'patriots': 1,\n",
              "                               'politics': 1,\n",
              "                               'powerful': 1,\n",
              "                               'precious': 1,\n",
              "                               'proclaim': 1,\n",
              "                               'profound': 1,\n",
              "                               'programs': 1,\n",
              "                               'progress': 1,\n",
              "                               'promises': 1,\n",
              "                               'question': 3,\n",
              "                               'reaching': 1,\n",
              "                               'reaffirm': 1,\n",
              "                               'relative': 1,\n",
              "                               'remained': 1,\n",
              "                               'remaking': 1,\n",
              "                               'remember': 2,\n",
              "                               'reminded': 1,\n",
              "                               'required': 1,\n",
              "                               'rightful': 1,\n",
              "                               'scarcely': 1,\n",
              "                               'security': 1,\n",
              "                               'services': 1,\n",
              "                               'settling': 1,\n",
              "                               'stairway': 1,\n",
              "                               'standing': 1,\n",
              "                               'stranger': 1,\n",
              "                               'strength': 1,\n",
              "                               'stronger': 2,\n",
              "                               'threaten': 1,\n",
              "                               'timeless': 1,\n",
              "                               'together': 1,\n",
              "                               'tolerate': 1,\n",
              "                               'traveled': 2,\n",
              "                               'unclench': 1,\n",
              "                               'ushering': 1,\n",
              "                               'violence': 1,\n",
              "                               'watchful': 1,\n",
              "                               'watching': 1,\n",
              "                               'weakened': 1,\n",
              "                               'weakness': 1}),\n",
              "                     9: FreqDist({'Americans': 3,\n",
              "                               'Arlington': 1,\n",
              "                               'President': 1,\n",
              "                               'Scripture': 1,\n",
              "                               'abandoned': 1,\n",
              "                               'advancing': 1,\n",
              "                               'alliances': 1,\n",
              "                               'alongside': 1,\n",
              "                               'ambitions': 2,\n",
              "                               'ancestors': 1,\n",
              "                               'apologize': 1,\n",
              "                               'arguments': 1,\n",
              "                               'believers': 1,\n",
              "                               'campfires': 1,\n",
              "                               'character': 1,\n",
              "                               'communism': 1,\n",
              "                               'curiosity': 1,\n",
              "                               'decisions': 1,\n",
              "                               'delivered': 1,\n",
              "                               'difficult': 1,\n",
              "                               'dignified': 1,\n",
              "                               'documents': 1,\n",
              "                               'factories': 1,\n",
              "                               'forgotten': 1,\n",
              "                               'gathering': 1,\n",
              "                               'gratitude': 1,\n",
              "                               'greatness': 2,\n",
              "                               'guardians': 1,\n",
              "                               'happiness': 1,\n",
              "                               'innocents': 1,\n",
              "                               'interests': 1,\n",
              "                               'inventive': 1,\n",
              "                               'knowledge': 2,\n",
              "                               'mountains': 1,\n",
              "                               'necessity': 1,\n",
              "                               'ourselves': 3,\n",
              "                               'patchwork': 1,\n",
              "                               'pleasures': 1,\n",
              "                               'political': 1,\n",
              "                               'precisely': 1,\n",
              "                               'qualities': 1,\n",
              "                               'resources': 1,\n",
              "                               'restraint': 1,\n",
              "                               'shortcuts': 1,\n",
              "                               'shuttered': 1,\n",
              "                               'silencing': 1,\n",
              "                               'something': 2,\n",
              "                               'strangled': 1,\n",
              "                               'struggled': 1,\n",
              "                               'suffering': 1,\n",
              "                               'tempering': 1,\n",
              "                               'tolerance': 1,\n",
              "                               'transform': 1,\n",
              "                               'uncertain': 1,\n",
              "                               'unmatched': 1}),\n",
              "                     10: FreqDist({'Christians': 1,\n",
              "                               'Gettysburg': 1,\n",
              "                               'businesses': 1,\n",
              "                               'celebrated': 1,\n",
              "                               'challenges': 2,\n",
              "                               'collective': 1,\n",
              "                               'confidence': 2,\n",
              "                               'corruption': 1,\n",
              "                               'everywhere': 1,\n",
              "                               'expedience': 1,\n",
              "                               'forbearers': 1,\n",
              "                               'foundation': 1,\n",
              "                               'generation': 5,\n",
              "                               'generosity': 1,\n",
              "                               'government': 3,\n",
              "                               'grievances': 1,\n",
              "                               'grudgingly': 1,\n",
              "                               'indicators': 1,\n",
              "                               'individual': 1,\n",
              "                               'inevitable': 1,\n",
              "                               'measurable': 1,\n",
              "                               'patriotism': 1,\n",
              "                               'principles': 1,\n",
              "                               'productive': 1,\n",
              "                               'prosperity': 3,\n",
              "                               'prosperous': 2,\n",
              "                               'protecting': 1,\n",
              "                               'restaurant': 1,\n",
              "                               'retirement': 1,\n",
              "                               'revolution': 1,\n",
              "                               'sacrificed': 1,\n",
              "                               'sacrifices': 1,\n",
              "                               'satisfying': 1,\n",
              "                               'statistics': 1,\n",
              "                               'strengthen': 1,\n",
              "                               'sweatshops': 1,\n",
              "                               'technology': 1,\n",
              "                               'themselves': 1,\n",
              "                               'throughout': 2,\n",
              "                               'tirelessly': 1,\n",
              "                               'transition': 1,\n",
              "                               'ultimately': 1,\n",
              "                               'understand': 2,\n",
              "                               'understood': 2,\n",
              "                               'unpleasant': 1}),\n",
              "                     11: FreqDist({'Afghanistan': 1,\n",
              "                               'adversaries': 1,\n",
              "                               'celebration': 1,\n",
              "                               'citizenship': 1,\n",
              "                               'consequence': 1,\n",
              "                               'convictions': 1,\n",
              "                               'cooperation': 2,\n",
              "                               'differences': 1,\n",
              "                               'firefighter': 1,\n",
              "                               'generations': 3,\n",
              "                               'governments': 1,\n",
              "                               'imagination': 1,\n",
              "                               'instruments': 1,\n",
              "                               'magnificent': 1,\n",
              "                               'opportunity': 1,\n",
              "                               'possessions': 1,\n",
              "                               'reaffirming': 1,\n",
              "                               'recognition': 1,\n",
              "                               'remembrance': 1,\n",
              "                               'responsibly': 1,\n",
              "                               'segregation': 1,\n",
              "                               'willingness': 2}),\n",
              "                     12: FreqDist({'indifference': 1,\n",
              "                               'presidential': 1,\n",
              "                               'selflessness': 1,\n",
              "                               'slaughtering': 1,\n",
              "                               'undiminished': 1,\n",
              "                               'universities': 1}),\n",
              "                     13: FreqDist({'determination': 1, 'understanding': 1}),\n",
              "                     14: FreqDist({'recriminations': 1, 'responsibility': 1}),\n",
              "                     16: FreqDist({'irresponsibility': 1})})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2roadSt2sRj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}